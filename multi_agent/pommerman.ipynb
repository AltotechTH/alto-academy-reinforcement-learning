{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-18 08:32:50.055 Python[22617:1033192] ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to /var/folders/rd/xnkdmhqx0c19zv0s0z9zcc9c0000gn/T/org.python.python.savedState\n",
      "/Users/thakorns/Desktop/AltoTech/codebases/alto-academy-reinforcement-learning/playground/pommerman/network/client/__init__.py:155: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if match_obj[0] is 0:\n",
      "/Users/thakorns/Desktop/AltoTech/codebases/alto-academy-reinforcement-learning/playground/pommerman/network/client/__init__.py:163: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif match_obj[0] is 2:\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "git clone https://github.com/MultiAgentLearning/playground.git\n",
    "cd playground\n",
    "pip install -e .\n",
    "\"\"\"\n",
    "\n",
    "import gym\n",
    "import pommerman\n",
    "from pommerman import agents\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explore Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_list = [\n",
    "    agents.SimpleAgent(),\n",
    "    agents.SimpleAgent(),\n",
    "    agents.SimpleAgent(),\n",
    "    agents.SimpleAgent()\n",
    "]\n",
    "env = pommerman.make('PommeFFACompetition-v0', agent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Discrete(6), Box(372,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space, env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env.render(mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = env.action_space.sample()\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [-1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "actions = env.action_space.sample()\n",
    "nobs, nreward, ndone, ninfo = env.step([actions]*4)\n",
    "print(actions, nreward)\n",
    "env.render(mode='human')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Learning Self-Play algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom environment for Pommerman\n",
    "class PommermanEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(PommermanEnv, self).__init__()\n",
    "        self.agent_list = [\n",
    "            agents.SimpleAgent(),\n",
    "            agents.SimpleAgent(),\n",
    "            agents.SimpleAgent(),\n",
    "            agents.SimpleAgent()\n",
    "        ]\n",
    "        self.env = pommerman.make('PommeFFACompetition-v0', self.agent_list)\n",
    "        self.observation_space = self.env.observation_space[0]\n",
    "        self.action_space = self.env.action_space[0]\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        return obs\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode=mode)\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SubprocVecEnv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Number of parallel environments\u001b[39;00m\n\u001b[1;32m      6\u001b[0m num_cpu \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m----> 7\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mSubprocVecEnv\u001b[49m([make_env \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_cpu)])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Initialize the PPO model\u001b[39;00m\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m'\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SubprocVecEnv' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize the custom Pommerman environment\n",
    "def make_env():\n",
    "    return PommermanEnv()\n",
    "\n",
    "# Number of parallel environments\n",
    "num_cpu = 4\n",
    "env = SubprocVecEnv([make_env for _ in range(num_cpu)])\n",
    "\n",
    "# Initialize the PPO model\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "\n",
    "# Training with self-play\n",
    "num_timesteps = int(1e6)\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path='./logs/',\n",
    "                                         name_prefix='ppo_pommerman')\n",
    "\n",
    "model.learn(total_timesteps=num_timesteps, callback=checkpoint_callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_pommerman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = PPO.load(\"ppo_pommerman\")\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate(model, num_episodes=10):\n",
    "    env = make_env()\n",
    "    total_rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "        total_rewards.append(total_reward)\n",
    "    avg_reward = sum(total_rewards) / num_episodes\n",
    "    print(f'Average reward over {num_episodes} episodes: {avg_reward}')\n",
    "    return avg_reward\n",
    "\n",
    "evaluate(model)\n",
    "\n",
    "# Play a game with the trained agent\n",
    "env = make_env()\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
